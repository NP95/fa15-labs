{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIXT33N version C\n",
    "## Phase 2: Principal Component Analysis\n",
    "\n",
    "### EE 16B: Designing Information Devices and Systems II, Fall 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name 1**:\n",
    "\n",
    "**Login**: ee16b-\n",
    "\n",
    "\n",
    "**Name 2**:\n",
    "\n",
    "**Login**: ee16b-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Table of Contents\n",
    "\n",
    "* [Introduction](#intro)\n",
    "* [Part 1: Data Collection](#part1)\n",
    "* [Part 2: Principal Component Analysis](#part2)\n",
    "* [Part 3: Classification](#part3)\n",
    "* [Part 4: Audio Link](#part4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "In this phase we will work out the mechanics of turning a gesture into a command that the Launchpad will execute.\n",
    "\n",
    "For this version of the project you will generate commands by drawing patterns using a mouse on a PC. There are five different commands that you will draw, at minimum. (If you want to implement more commands then go for it!)\n",
    "\n",
    "- Straight vertical line from bottom to top = Fast\n",
    "- Straight vertical line from top to bottom = Slow\n",
    "- Straight horizontal line from right to left = Turn Left\n",
    "- Straight horizontal line from left to right = Turn Right\n",
    "- Clockwise circle starting at 12 'o clock = Party Mode\n",
    "\n",
    "You will draw these shapes in a Python canvas on your computer, so all of your PCA training and classification will run on your PC. Based on the classifier result, the PC will then transmit an audio signal, which will be picked up by the Launchpad.\n",
    "\n",
    "The goals of this phase are as follows:\n",
    "- Collect gesture data\n",
    "- PCA + Classifier (PC, 5 commands)\n",
    "- Check accuracy\n",
    "- Send resulting command through audio link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1'></a>\n",
    "##<span style=\"color:blue\">Part 1: Data Collection</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start our classifier training, we need to first gather some data. We have developed a simple script that allows you to collect the xy-coordinates as you draw on a canvas. To use this script, run\n",
    "\n",
    "<b>`python capture.py log.csv`</b>\n",
    "\n",
    "on the command line. This will bring up a white canvas where you can draw a pattern. Start by drawing a line from left to right 20 times in the box. Now, you can obtain the data by looking for a file called <b>`log.csv`</b>. If this was the data that you want to use for your \"Right\" command, rename the file as <b>`right.csv`</b>.\n",
    "\n",
    "Repeat the process for <b>`left.csv`</b>, <b>`up.csv`</b>, <b>`down.csv`</b> and <b>`circle.csv`</b>. Remember to rename the file before running the script again if you want to discard the last run since the script appends to the file.\n",
    "\n",
    "Make sure you have 20 data points for each of 5 different gestures: right, left, up, down and clockwise circle. To get a robust training set, vary your gestures a bit by doing it in different parts of the canvas and in different sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2'></a>\n",
    "##<span style=\"color:blue\">Part 2: Principal Component Analysis</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have some data, you can apply PCA to classify the different gestures. Run through the cells below to set iPython up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    \"\"\"\n",
    "    Reads a csv file and returns the first 20 recordings from the file\n",
    "    Input:\n",
    "        filename: csv filename\n",
    "    Output:\n",
    "        data: a 20x66 matrix corresponding to the first 20 readings in the csv file. Each row corresponds\n",
    "            to a reading; the first 33 values are x-coordinates while the second33 values are y-coordinates\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for i,row in enumerate(reader):\n",
    "            data.append(np.array([float(i) for i in row]).T)\n",
    "    data = np.array(data)\n",
    "    data = np.hstack((data[::2,:], data[1::2,:]))\n",
    "    data = data[:20,:] # Take only first 20 readings\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the <b>`read_csv`</b> function above, build the <b>`A`</b> matrix for PCA. The function <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html\"><b>`np.hstack`</b></a> might be helpful here. Then plot the data using the <b>`plot_data`</b> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load your 5 csv files and stack them into a 100x66 numpy array using the read_csv function\n",
    "data = read_csv('up.csv')\n",
    "data = np.vstack((data, read_csv('down.csv')))\n",
    "data = np.vstack((data, read_csv('left.csv')))\n",
    "data = np.vstack((data, read_csv('right.csv')))\n",
    "data = np.vstack((data, read_csv('circle.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_data(d):\n",
    "    \"\"\"\n",
    "    Plots data in the right canvas coordinates\n",
    "    Input:\n",
    "        d: Nx66 data array\n",
    "    Output:\n",
    "        plots N curves on the same graph\n",
    "    \"\"\"\n",
    "    plt.gca().invert_yaxis()\n",
    "    for line in d:\n",
    "        l = len(line)\n",
    "        plt.plot(line[:l/2], line[l/2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See the gesture paths you recorded\n",
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot above, PCA might have some trouble classifying since the data is not normalized in any way. To solve this issue, implement a 'normalization' scheme where you center each gesture recording to (0,0). To do this, first find the mean of the x-coordinates and y-coordinates, then simply subtract that mean from each point. Remember that the first 33 elements of each row are the x-coordinates and the second 33 elements of each row are the y-coordinates; you have to normalize them separately. Your data should look similar to the plot below.\n",
    "\n",
    "<center>\n",
    "<img width='400px' src=\"http://inst.eecs.berkeley.edu/~ee16b/fa15/lab_pics/proj-gesture-norm.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize your data so it is more suitable for PCA\n",
    "data_norm = np.copy(data)\n",
    "# YOUR CODE HERE\n",
    "for i in range(len(data_norm)):\n",
    "    data_norm[i,:] =\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot normalized data\n",
    "plot_data(data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot each component of normalized data separately\n",
    "plt.plot(data_norm[:,:33].T)\n",
    "plt.title('x-coordinate data points')\n",
    "plt.figure()\n",
    "plt.plot(data_norm[:,33:].T)\n",
    "plt.show()\n",
    "plt.title('y-coordinate data points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can try using SVD to retrieve the principal components. After you have done so, plot the sigma values. If they are not satisfactory, think of other ways you can normalize the data and modify the cells above until you are satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call SVD on the normalized data matrix\n",
    "[u,s,v] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the sigma values\n",
    "plt.stem(s)\n",
    "plt.xlim([-0.5,10])\n",
    "plt.title('Sigma values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">How many principal components are significant?</span>**\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot out the significant principal components you found above and project the data on the new space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the significant principal components\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Project the data matrix on to the first 3 principal components\n",
    "# YOUR CODE HERE\n",
    "proj = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data with only 2 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the projection on the first 2 principal components\n",
    "n = 20 # Number of recordings of each gesture\n",
    "plt.scatter(proj[:n,0], proj[:n,1], c=['red'], edgecolor='None')\n",
    "plt.scatter(proj[n:2*n,0], proj[n:2*n,1], c=['blue'], edgecolor='None')\n",
    "plt.scatter(proj[2*n:3*n,0], proj[2*n:3*n,1], c=['green'], edgecolor='None')\n",
    "plt.scatter(proj[3*n:4*n,0], proj[3*n:4*n,1], c=['cyan'], edgecolor='None')\n",
    "plt.scatter(proj[4*n:5*n,0], proj[4*n:5*n,1], c=['black'], edgecolor='None')\n",
    "plt.legend(['up', 'down', 'left', 'right', 'circle'],loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot with all 3 principal components we calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the projection on the first 3 principal components\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "n=20\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(proj[:n,0], proj[:n,1], proj[:n,2], c=['red'], edgecolor='None')\n",
    "ax.scatter(proj[n:2*n,0], proj[n:2*n,1], proj[n:2*n,2], c=['blue'], edgecolor='None')\n",
    "ax.scatter(proj[2*n:3*n,0], proj[2*n:3*n,1], proj[2*n:3*n,2], c=['green'], edgecolor='None')\n",
    "ax.scatter(proj[3*n:4*n,0], proj[3*n:4*n,1], proj[3*n:4*n,2], c=['cyan'], edgecolor='None')\n",
    "ax.scatter(proj[4*n:5*n,0], proj[4*n:5*n,1], proj[4*n:5*n,2], c=['black'], edgecolor='None')\n",
    "\n",
    "# Point of view - modify this to move around the camera position\n",
    "ax.view_init(elev=30, azim=45) \n",
    "\n",
    "ax.legend(['up', 'down', 'left', 'right', 'circle'],loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try looking at the data from another angle by replacing the ax.view_init line with: <b>`ax.view_init(elev=0, azim=45)`</b>. This provides a different camera position so you can get a better idea of how multiple principle components classify your data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">After looking at the plots above, how many principal components would you choose to make your classification easier?</span>**\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above should be very easy to classify. If you do not see nice clustering, try to re-record the data with straighter lines. If it still fails, seek help from a GSI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3'></a>\n",
    "##<span style=\"color:blue\">Part 3: Classification</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the plots above, we will define a way of classifying the different gestures. Fill in the skeleton code below to determine the gesture of a new reading vector and try out the classification function. Don't forget to do the same normalization to the vector as we are feeding in raw data.\n",
    "\n",
    "Note the colors for each gesture in the legend of the plots above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(vector, new_basis):\n",
    "    \"\"\"\n",
    "    Classifies a new reading vector into one of the 5 gestures.\n",
    "    Inputs:\n",
    "        vector: 1x66 reading vector - first 33 elements correspond to x-coordinates\n",
    "            and second 33 elements correspond to y-coordinates\n",
    "        new_basis: Nx66 matrix with the basis of the new space, where N is the number\n",
    "            of principal components used\n",
    "    Output:\n",
    "        String of the classified gesture names\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    proj = \n",
    "    if (...):\n",
    "        return 'up'\n",
    "    if (...):\n",
    "        return 'down'\n",
    "    if (...):\n",
    "        return 'left'\n",
    "    if (...):\n",
    "        return 'right'\n",
    "    if (...):\n",
    "        return 'circle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try out the classification function\n",
    "print(classify(data[0,:], ...)) # Modify to use other vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part4'></a>\n",
    "##<span style=\"color:blue\">Part 4: Audio Link</span>\n",
    "\n",
    "###Materials\n",
    "- Microphone front-end circuit\n",
    "- Launchpad + USB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from last time, to send some commands from the PC to the Launchpad, we will create a very basic audio channel using the PC speakers and the microphone circuit you just built. Just like On-Off Keyeing (OOK) in the wireless module in EE16A, we will modulate our command on a sinusoid. In our case, we will use a 1kHz sinusoid since it is in the best frequency range of the microphone. The command will be encoded in the time between 2 pulses. For example a sinusoid for 0.1s continued with nothing for 0.1s and then another sinusoid for 0.1s encodes a certain command. If the time of the empty space between the two pulses is 0.3s instead, it encodes a different command.\n",
    "\n",
    "Now we will incorporate the PCA classification with the audio link. The code from the last phase is reproduced below with a little modification. Upload the sketch <b>`classify.ino`</b> to the Launchpad. \n",
    "\n",
    "Make sure that the microphone circuit from the last phase is still working. Remember that the first op-amp is powered from the voltage regulator while the second op-amp is powered by the Launchpad's 3.3V pin. Probe the output voltage of the front end circuit and make sure the DC level is around 1.6V and the signal saturates at 0V and 3.3V.\n",
    "\n",
    "Once everything is set up, open the Serial Monitor in Energia and run the cells below. The Launchpad will print out the gesture if it is able to recognize the audio command. Try calling your classify function on one of the test vectors that you created. The audio code will output the appropriate pulses and send the command to the Launchpad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "def play_audio( data, p, fs):\n",
    "    \"\"\"\n",
    "    Plays audio using pyAudio\n",
    "    Parameters:\n",
    "        data: audio data array\n",
    "        p: pyAudio object\n",
    "        fs: sampling rate\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    ostream = p.open(format=pyaudio.paFloat32, channels=1, rate=fs,output=True)\n",
    "    ostream.write( data.astype(np.float32).tostring() )\n",
    "\n",
    "def generate_pulses(mask, fs, f, length):\n",
    "    \"\"\" \n",
    "    Generate audio encoding\n",
    "    Parameters:\n",
    "        mask: List containing audio mask (on-off)\n",
    "        fs: sampling frequency\n",
    "        f: carrier frequency\n",
    "        length: length of each pulse/entry in mask\n",
    "    Returns:\n",
    "        Numpy array containing encoded data\n",
    "    \"\"\"\n",
    "    end = len(mask)*length\n",
    "    mask = np.repeat(mask, fs*length)\n",
    "    x = np.linspace(0,end,len(mask))\n",
    "    data = np.sin(2*pi*f*x)\n",
    "    return data * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = 44100\n",
    "f = 1000\n",
    "\n",
    "one = generate_pulses([1, 0, 1, 0], fs, f, 0.1)  # Pulses of 0.1s apart\n",
    "two = generate_pulses([1, 0, 0, 1, 0], fs, f, 0.1)  # Pulses of 0.2s apart\n",
    "three = generate_pulses([1, 0, 0, 0, 1, 0], fs, f, 0.1)  # Pulses of 0.3s apart\n",
    "four = generate_pulses([1, 0, 0, 0, 0, 1, 0], fs, f, 0.1)  # Pulses of 0.4s apart\n",
    "five = generate_pulses([1, 0, 0, 0, 0, 0, 1, 0], fs, f, 0.1)  # Pulses of 0.5s apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Call your classify function with data that you want to test\n",
    "gesture = classify(...)\n",
    "\n",
    "if (gesture == 'up'):\n",
    "    play_audio(one, p, fs )\n",
    "if (gesture == 'down'):\n",
    "    play_audio(two, p, fs )\n",
    "if (gesture == 'left'):\n",
    "    play_audio(three, p, fs )\n",
    "if (gesture == 'right'):\n",
    "    play_audio(four, p, fs )\n",
    "if (gesture == 'circle'):\n",
    "    play_audio(five, p, fs )\n",
    "\n",
    "p.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
